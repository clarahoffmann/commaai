{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helpers as hlp\n",
    "import math\n",
    "from random import random, seed\n",
    "from numpy.linalg import multi_dot\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\"\n",
    "        src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML\"></script>\n",
    "\n",
    "## Model\n",
    "Linear model\n",
    "\\\\[ \\tilde{\\boldsymbol{Z}} = B_{\\boldsymbol{\\zeta}}(\\boldsymbol{x})\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\, \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2I) \\\\]\n",
    "\n",
    "and the transformed target variables follow a conditional normal distribution\n",
    "\n",
    "\\\\[ \\boldsymbol{Z} | \\boldsymbol{x}, \\sigma^2, \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{0}, R(\\boldsymbol{x}, \\boldsymbol{\\theta})^T) \\\\]\n",
    "with $ R(\\boldsymbol{x}, \\boldsymbol{\\theta}) = S(\\boldsymbol{x}, \\boldsymbol{\\theta})(I - B\\Omega B^T)^{-1}S(\\boldsymbol{x}, \\boldsymbol{\\theta})^T $\n",
    "\n",
    "and each Z_i has a marginal standard-normal distribution.\n",
    "\n",
    "The coefficient vector beta follows a conditional normal distribution\n",
    "\n",
    "\\\\[ \\boldsymbol{\\beta} | \\boldsymbol{x}, \\sigma^2, \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 P(\\boldsymbol{\\theta})^{-1}) \\\\]\n",
    "\n",
    "\n",
    "## Prior on copula parameters\n",
    "\n",
    "Horseshoe prior on coefficients\n",
    "\\\\[\\beta_j| \\lambda_j \\sim \\mathcal{N}(0,\\lambda_j^2) \\\\]\n",
    "with $\\pi_0(\\lambda_j | \\tau) = C^{+}(0,\\tau) $ and $ \\pi_0(\\tau) = C^{+}(0,1)$, where $C^{+}$ is the half-Cauchy distribution\n",
    "\n",
    "Then the vector of copula parameters is\n",
    "\\\\[\\boldsymbol{\\theta} = \\{ \\boldsymbol{\\lambda}, \\tau \\} \\\\]\n",
    "with \n",
    "\\\\[ \\boldsymbol{\\lambda} = (\\lambda_1,...\\lambda_p)^T \\\\]\n",
    "\n",
    "\\begin{equation}\n",
    "   P(\\boldsymbol{\\theta}) = diag(\\lambda_1^2,...\\lambda_p^2)^{-1}\n",
    "\\end{equation}\n",
    "and \n",
    "\\\\[ R(\\boldsymbol{x}, \\boldsymbol{\\theta}) = S(\\boldsymbol{x}, \\boldsymbol{\\theta}(I + B \\mathrm{diag}(\\lambda_1, ... \\lambda_p)^2 B ^T)S(\\boldsymbol{x}, \\boldsymbol{\\theta}) \\\\]\n",
    "\n",
    "\n",
    " - > so is P(theta) then just diag(phi_i)?\n",
    "Distribution of targets y\n",
    "\n",
    "\\\\[ p(\\boldsymbol{y}| \\boldsymbol{x}, \\boldsymbol{\\beta}, \\boldsymbol{\\theta}) = \\phi_n(\\boldsymbol{z};S(\\boldsymbol{x}, \\boldsymbol{\\theta})B_{\\boldsymbol{\\zeta}}\\boldsymbol{\\beta}, S(\\boldsymbol{x}, \\boldsymbol{\\theta})^2) \\prod_{i=1}^n \\frac{p_Y(y_i)}{\\phi_1(z_i)}, \\\\]\n",
    "with\n",
    "\n",
    "\\\\[ S(\\boldsymbol{x}, \\boldsymbol{\\theta}) = diag(s_1,...,s_n) \\\\]\n",
    "\n",
    "with $ s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^TP(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}}) $\n",
    "\n",
    "and specifically for the horseshoe prior case:\n",
    "\\\\[s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^T P(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}} \\\\]\n",
    "\n",
    "\n",
    "Now we want to optimize the ELBO which is given by\n",
    "\\\\[ \\mathcal{L}(\\lambda) = \\mathbb{E}_q[\\log h(\\vartheta) - \\log q_{\\lambda}(\\vartheta) ] \\\\]\n",
    "\n",
    "where $\\vartheta = \\{\\beta, \\theta \\}$ and $h(\\vartheta) = p(\\vartheta)p(y|\\vartheta)$.\n",
    "We want to estimate the augmented posteriors $\\beta, \\theta | \\boldsymbol{y}$\n",
    "\n",
    "\n",
    "## VA with factor covariance structure\n",
    "\n",
    "Choose an approximating family $ q_{\\lambda}(\\vartheta) $, in our case this is \n",
    "\\begin{equation}\n",
    "q_{\\lambda}(\\vartheta) = \\mathcal{N}(\\boldsymbol{\\mu}, BB^T + D^2),\n",
    "\\end{equation}\n",
    "with $d = \\{d_1,...d_m \\}$. The dimension of this distribution is $m$ and in the case of the horseshoe prior this is # of betas + 1 (tau) $= p+1$. The dimension of $B$ is $m \\times k$, where $k$ specifies the number of factors. To make computation easier $k$ should be much smaller than $m$. This implies that we can represent the dependency structure in the covariance matrix with a smaller number of latent variables, thus facilitating faster computation.\n",
    "\n",
    "How can we draw from this distribution?\n",
    "- first draw $(\\boldsymbol{z}, \\boldsymbol{\\epsilon}) \\sim \\mathcal{N}(0,I)$, where $\\boldsymbol{z}$ is of dimension $k \\times 1 $ and $\\boldsymbol{\\epsilon}$ is $m \\times 1$\n",
    "- then calculate $\\vartheta = \\boldsymbol{\\mu} + B\\boldsymbol{z} + d \\circ \\epsilon$, where $\\circ$ denotes the Hadamard product, i.e. the element by element multiplication of two vectors.\n",
    "\n",
    "By applying the reparametrization trick we can now change the  expectation with regard to $q_{\\lambda}(\\theta)$, namely $\\mathbb{E}_q$ to an expectation with regard to standard normal density of $z, \\epsilon$, which is denoted as $f(z, \\epsilon)$, which leads to the expectation $\\mathbb{E}_f$.\n",
    "Instead of evaluating the first time of the ELBO at $\\theta$, we evaluate it at the reparametarized $\\theta = \\mu + Bz + d \\circ \\epsilon$.\n",
    "The expectation with regard to $f$, $\\mathbb{E}_f$, can be estimated unbiasedly by generating one or more samples from $f$.\n",
    "\n",
    "Calculating the gradients delivers:\n",
    "- gradient w.r.t $\\mu$, i.e. mean of variational parameters $\\nabla_{\\mu} \\mathcal{L}(\\lambda) = \\mathbb{E}_f[\\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon)]$\n",
    "- gradient w.r.t. $B$, i.e. first component of covariance matrix of variational parameters $\\nabla_B \\mathcal{L}(\\lambda) = \\mathbb{E}_f[ \\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon)z^T + (BB^T + D^2)^{-1}(Bz+d \\circ \\epsilon)z^T]$\n",
    "- gradient w.r.t. $D = \\mathrm{diag}(d_1,...,d_n)$, i.e. the second component of covariance matrix of variational parameters $\\nabla_d \\mathcal{L}(\\lambda) = \\mathbb{E}_f[\\mathrm{diag}(\\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon) \\epsilon^T + (BB^T + D^2)^{-1}(Bz + d \\circ \\epsilon) \\epsilon^T] $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Initialize $\\lambda = \\lambda^{(0)} = (\\mu^{(0)}, B^{(0)}, d^{(0)})$, $t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load variables from training DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_coefficients_path = '../../../data/commaai/extracted_coefficients/20201021_unrestr_gaussian_resampled/'\n",
    "B_zeta_path = str(extracted_coefficients_path + 'Bzeta/B_zeta.npy')\n",
    "beta_path = str(extracted_coefficients_path + 'beta/beta.csv')\n",
    "z_path = str(extracted_coefficients_path + 'Bzeta/tr_labels.npy')\n",
    "\n",
    "beta = np.genfromtxt(beta_path, delimiter=',')\n",
    "# B_zeta is a n x q matrix\n",
    "B_zeta = np.load(B_zeta_path)\n",
    "B_zeta = B_zeta.reshape(B_zeta.shape[0], beta.shape[0])\n",
    "z = np.load(z_path) #[0:B_zeta.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p is the number of beta coefficients in the last hidden layer\n",
    "p = B_zeta.shape[1]\n",
    "\n",
    "\n",
    "# Lambda is a diagonal matrix of dimension p\n",
    "Lambda = np.diag(np.random.rand(p,))\n",
    "\n",
    "seed(679305)\n",
    "tau = random()\n",
    "\n",
    "# Set iteration counter to 0\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355543, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_zeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = B_zeta.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[ S(\\boldsymbol{x}, \\boldsymbol{\\theta}) = diag(s_1,...,s_n) \\\\]\n",
    "\n",
    "with $ s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^TP(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S(x, theta) is of dimension n x n\n",
    "S = np.array([(1 + ((B_zeta[i,:].T).dot(Lambda)).dot(B_zeta[i,:]))**(-1/2) for i in range(0,n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialize $\\lambda = \\lambda^{(0)} = (\\mu^{(0)},B^{(0)},d^{(0)}), \\, t = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m is number of variational parameters, which is \n",
    "# 2p (for each lambda_j and each beta_j)\n",
    "# plus the variational parameter for the prior on lambda\n",
    "m = 2*p + 1\n",
    "\n",
    "# number of factors in the factored covariance representation\n",
    "k = m - 5\n",
    "\n",
    "mu_t = np.array([random() for i in range(0,m)]).reshape(m,1)\n",
    "# B is a lower triangle m x k matrix and is the first component of the \n",
    "# covariance matrix\n",
    "B_t = np.tril(np.random.rand(m,k))\n",
    "while not np.linalg.matrix_rank(B_t) == k:\n",
    "    B_t = np.tril(np.random.rand(m,k))\n",
    "\n",
    "# D is a diagonal matrix of dimension m x m and is the second component of the \n",
    "# covariance matrix\n",
    "D_t = np.diag(np.random.rand(m,))\n",
    "d_t = np.diag(D_t).reshape(m,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate $(\\epsilon^{(t)}, z^{(t)}) \\sim \\mathcal{N}(0,I) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_epsilon = np.repeat(0, m)\n",
    "mean_z = np.repeat(0, k)\n",
    "\n",
    "var_epsilon = np.diag(np.repeat(1,m))\n",
    "var_z = np.diag(np.repeat(1,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adadelta\n",
    "decay_rate = 0.95\n",
    "constant = 1e-7\n",
    "E_g2_t_1 = 0\n",
    "E_delta_x_2_1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_g2_t_1_mu = np.repeat(0, len(mu_t))\n",
    "E_delta_x_2_1_mu = np.repeat(0, len(mu_t))\n",
    "E_g2_t_1_B = np.zeros(B_t.shape)\n",
    "E_delta_x_2_1_B = np.zeros(B_t.shape)\n",
    "E_g2_t_1_d = np.repeat(0, len(d_t)).reshape(m,1)\n",
    "E_delta_x_2_1_d = np.repeat(0, len(d_t)).reshape(m,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adadelta_change(gradient, E_g2_t_1, E_delta_x_2_1, decay_rate = 0.99, constant = 10e-6):\n",
    "    # expected squared gradient for next iteration\n",
    "    E_g2_t = decay_rate*E_g2_t_1 + (1 - decay_rate)*(gradient**2)\n",
    "    # update for parameter\n",
    "    # should there be a minus or plus here ?????\n",
    "    delta_x =  (np.sqrt(E_delta_x_2_1 + constant)/np.sqrt(E_g2_t + constant))*gradient\n",
    "    # expected update for next iteration\n",
    "    E_delta_x_2 = decay_rate*E_delta_x_2_1 + (1 - decay_rate)*(delta_x**2)\n",
    "    return(delta_x, E_g2_t, E_delta_x_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/50000 [04:39<204:09:13, 14.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c5a6f925e965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# 3. Compute gradient of beta, lambda_j, and tau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mgradient_h_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDelta_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvartheta_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_zeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Compute inverse with Woodbury formula.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/commaai_code/04b_VA/filtered_gaussian_resampled/helpers.py\u001b[0m in \u001b[0;36mDelta_theta\u001b[0;34m(vartheta_t, B_zeta, n, z, p)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# update S for lambda values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mS_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_zeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambda_t\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_zeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Gradient w.r.t. beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/commaai_code/04b_VA/filtered_gaussian_resampled/helpers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# update S for lambda values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mS_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_zeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambda_t\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_zeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Gradient w.r.t. beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lower_bounds = []\n",
    "all_varthetas = []\n",
    "t = 0\n",
    "iterations = 50000\n",
    "for i in tqdm(range(iterations)):\n",
    "    \n",
    "    # 1. Generate epsilon_t and z_t\n",
    "    z_t = hlp.generate_z(mean_z,var_z)\n",
    "    epsilon_t = hlp.generate_epsilon(mean_epsilon, var_epsilon)\n",
    "    \n",
    "    # 2. Draw from vartheta, what we generate are log values\n",
    "    # of lambda and tau -> have to transform them back to use them\n",
    "    vartheta_t = mu_t + B_t.dot(z_t) + (d_t*epsilon_t)\n",
    "    \n",
    "    # transform lambda and tau to log values\n",
    "    # rather do in method to keep updates correct\n",
    "    #vartheta_t[p:2*p] = np.sqrt(np.exp(vartheta_t[p:2*p]**2))\n",
    "    #vartheta_t[2*p] = np.exp(abs(vartheta_t[2*p]))\n",
    "    \n",
    "    # 3. Compute gradient of beta, lambda_j, and tau\n",
    "    gradient_h_t = hlp.Delta_theta(vartheta_t, B_zeta, n, z, p)\n",
    "    \n",
    "    # Compute inverse with Woodbury formula.\n",
    "    inv = np.linalg.inv(D_t.dot(D_t))\n",
    "    inv2 = np.linalg.inv(np.identity(k) + B_t.T.dot(inv).dot(B_t))\n",
    "    BBD_inv = inv - multi_dot([inv, B_t, inv2, B_t.T, inv])\n",
    "    \n",
    "    # Compute gradients for the variational parameters mu, B, D\n",
    "    Delta_mu = hlp.Delta_mu(gradient_h_t, BBD_inv, z_t, d_t, epsilon_t, B_t)\n",
    "    Delta_B = hlp.Delta_B(B_zeta,n,z, p, B_t, gradient_h_t, z_t, D_t, d_t, epsilon_t, BBD_inv)\n",
    "    Delta_D = hlp.Delta_D(gradient_h_t, epsilon_t,D_t, d_t,p, BBD_inv)\n",
    "    \n",
    "    # 4. Adadelta Updates\n",
    "    update_mu, E_g2_t_1_mu, E_delta_x_2_1_mu = adadelta_change(Delta_mu, E_g2_t_1_mu, E_delta_x_2_1_mu, decay_rate = decay_rate, constant = constant)\n",
    "    update_B, E_g2_t_1_B, E_delta_x_2_1_B  = adadelta_change(Delta_B, E_g2_t_1_B, E_delta_x_2_1_B, decay_rate = decay_rate, constant = constant)\n",
    "    update_d, E_g2_t_1_d, E_delta_x_2_1_d = adadelta_change(Delta_D, E_g2_t_1_d, E_delta_x_2_1_d, decay_rate = decay_rate, constant = constant)\n",
    "    \n",
    "    # Update variables\n",
    "    '''rho = 0.9\n",
    "    mu_t = mu_t + rho*Delta_mu.reshape(m,1)\n",
    "    B_t = B_t + rho*Delta_B\n",
    "    B_t *= np.tri(*B_t.shape)\n",
    "    d_t = (d_t + rho*Delta_D)\n",
    "    D_t = np.diag(d_t.reshape(m,))'''\n",
    "    mu_t = mu_t + update_mu.reshape(m,1)\n",
    "    B_t = B_t + update_B\n",
    "    # set upper triangular elements to 0\n",
    "    B_t *= np.tri(*B_t.shape)\n",
    "    d_t = (d_t + update_d)\n",
    "    D_t = np.diag(d_t.reshape(m,))\n",
    "    \n",
    "    vartheta_t = mu_t + B_t.dot(z_t) + (d_t*epsilon_t)\n",
    "    vartheta_t_transf = vartheta_t.copy()\n",
    "    # 5. compute stopping criterion\n",
    "    beta_t = vartheta_t_transf[0:p].reshape(p,)\n",
    "    Lambda_t = np.diag(np.sqrt(np.exp(vartheta_t_transf[p:2*p].reshape(p,))))\n",
    "    tau_t = np.exp(vartheta_t_transf[2*p])\n",
    "    \n",
    "    # Lower bound L(lambda) = E[log(L_lambda - q_lambda]\n",
    "    log_h_t = hlp.log_density(z, beta_t, B_zeta, Lambda_t, p, abs(tau_t),n)\n",
    "    log_q_lambda_t = np.log(hlp.multivariate_normal(vartheta_t, m, mu_t, (B_t.dot(B_t.T) + D_t**2)))\n",
    "    # evidence lower bound\n",
    "    L_lambda = log_h_t - log_q_lambda_t\n",
    "    lower_bounds.append(L_lambda.item())\n",
    "    all_varthetas.append(L_lambda.item())\n",
    "    \n",
    "    # increase time count\n",
    "    t = t+1\n",
    "    \n",
    "    # can also set lambda as the value over the last 10 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../../data/density/03082020/lower_lambda_va.csv', lower_bounds, delimiter=\",\")\n",
    "np.savetxt('../../data/density/03082020/vartheta_final.csv', vartheta_t, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check convergence graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0,t), lower_bounds, linewidth=0.1)\n",
    "plt.yscale('symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lower_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save $\\vartheta$ over last 10% of runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_10_percent = iterations*0.01\n",
    "vartheta_hat = mean(all_varthetas[last_10_percent:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../../data/bdd100k/density/03082020/vartheta_hat.csv', vartheta_hat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
