{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helpers as hlp\n",
    "import math\n",
    "from random import random, seed\n",
    "from numpy.linalg import multi_dot\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\"\n",
    "        src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML\"></script>\n",
    "\n",
    "## Model\n",
    "Linear model\n",
    "\\\\[ \\tilde{\\boldsymbol{Z}} = B_{\\boldsymbol{\\zeta}}(\\boldsymbol{x})\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\, \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2I) \\\\]\n",
    "\n",
    "and the transformed target variables follow a conditional normal distribution\n",
    "\n",
    "\\\\[ \\boldsymbol{Z} | \\boldsymbol{x}, \\sigma^2, \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{0}, R(\\boldsymbol{x}, \\boldsymbol{\\theta})^T) \\\\]\n",
    "with $ R(\\boldsymbol{x}, \\boldsymbol{\\theta}) = S(\\boldsymbol{x}, \\boldsymbol{\\theta})(I - B\\Omega B^T)^{-1}S(\\boldsymbol{x}, \\boldsymbol{\\theta})^T $\n",
    "\n",
    "and each Z_i has a marginal standard-normal distribution.\n",
    "\n",
    "The coefficient vector beta follows a conditional normal distribution\n",
    "\n",
    "\\\\[ \\boldsymbol{\\beta} | \\boldsymbol{x}, \\sigma^2, \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 P(\\boldsymbol{\\theta})^{-1}) \\\\]\n",
    "\n",
    "\n",
    "## Prior on copula parameters\n",
    "\n",
    "Horseshoe prior on coefficients\n",
    "\\\\[\\beta_j| \\lambda_j \\sim \\mathcal{N}(0,\\lambda_j^2) \\\\]\n",
    "with $\\pi_0(\\lambda_j | \\tau) = C^{+}(0,\\tau) $ and $ \\pi_0(\\tau) = C^{+}(0,1)$, where $C^{+}$ is the half-Cauchy distribution\n",
    "\n",
    "Then the vector of copula parameters is\n",
    "\\\\[\\boldsymbol{\\theta} = \\{ \\boldsymbol{\\lambda}, \\tau \\} \\\\]\n",
    "with \n",
    "\\\\[ \\boldsymbol{\\lambda} = (\\lambda_1,...\\lambda_p)^T \\\\]\n",
    "\n",
    "\\begin{equation}\n",
    "   P(\\boldsymbol{\\theta}) = diag(\\lambda_1^2,...\\lambda_p^2)^{-1}\n",
    "\\end{equation}\n",
    "and \n",
    "\\\\[ R(\\boldsymbol{x}, \\boldsymbol{\\theta}) = S(\\boldsymbol{x}, \\boldsymbol{\\theta}(I + B \\mathrm{diag}(\\lambda_1, ... \\lambda_p)^2 B ^T)S(\\boldsymbol{x}, \\boldsymbol{\\theta}) \\\\]\n",
    "\n",
    "\n",
    " - > so is P(theta) then just diag(phi_i)?\n",
    "Distribution of targets y\n",
    "\n",
    "\\\\[ p(\\boldsymbol{y}| \\boldsymbol{x}, \\boldsymbol{\\beta}, \\boldsymbol{\\theta}) = \\phi_n(\\boldsymbol{z};S(\\boldsymbol{x}, \\boldsymbol{\\theta})B_{\\boldsymbol{\\zeta}}\\boldsymbol{\\beta}, S(\\boldsymbol{x}, \\boldsymbol{\\theta})^2) \\prod_{i=1}^n \\frac{p_Y(y_i)}{\\phi_1(z_i)}, \\\\]\n",
    "with\n",
    "\n",
    "\\\\[ S(\\boldsymbol{x}, \\boldsymbol{\\theta}) = diag(s_1,...,s_n) \\\\]\n",
    "\n",
    "with $ s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^TP(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}}) $\n",
    "\n",
    "and specifically for the horseshoe prior case:\n",
    "\\\\[s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^T P(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}} \\\\]\n",
    "\n",
    "\n",
    "Now we want to optimize the ELBO which is given by\n",
    "\\\\[ \\mathcal{L}(\\lambda) = \\mathbb{E}_q[\\log h(\\vartheta) - \\log q_{\\lambda}(\\vartheta) ] \\\\]\n",
    "\n",
    "where $\\vartheta = \\{\\beta, \\theta \\}$ and $h(\\vartheta) = p(\\vartheta)p(y|\\vartheta)$.\n",
    "We want to estimate the augmented posteriors $\\beta, \\theta | \\boldsymbol{y}$\n",
    "\n",
    "\n",
    "## VA with factor covariance structure\n",
    "\n",
    "Choose an approximating family $ q_{\\lambda}(\\vartheta) $, in our case this is \n",
    "\\begin{equation}\n",
    "q_{\\lambda}(\\vartheta) = \\mathcal{N}(\\boldsymbol{\\mu}, BB^T + D^2),\n",
    "\\end{equation}\n",
    "with $d = \\{d_1,...d_m \\}$. The dimension of this distribution is $m$ and in the case of the horseshoe prior this is # of betas + 1 (tau) $= p+1$. The dimension of $B$ is $m \\times k$, where $k$ specifies the number of factors. To make computation easier $k$ should be much smaller than $m$. This implies that we can represent the dependency structure in the covariance matrix with a smaller number of latent variables, thus facilitating faster computation.\n",
    "\n",
    "How can we draw from this distribution?\n",
    "- first draw $(\\boldsymbol{z}, \\boldsymbol{\\epsilon}) \\sim \\mathcal{N}(0,I)$, where $\\boldsymbol{z}$ is of dimension $k \\times 1 $ and $\\boldsymbol{\\epsilon}$ is $m \\times 1$\n",
    "- then calculate $\\vartheta = \\boldsymbol{\\mu} + B\\boldsymbol{z} + d \\circ \\epsilon$, where $\\circ$ denotes the Hadamard product, i.e. the element by element multiplication of two vectors.\n",
    "\n",
    "By applying the reparametrization trick we can now change the  expectation with regard to $q_{\\lambda}(\\theta)$, namely $\\mathbb{E}_q$ to an expectation with regard to standard normal density of $z, \\epsilon$, which is denoted as $f(z, \\epsilon)$, which leads to the expectation $\\mathbb{E}_f$.\n",
    "Instead of evaluating the first time of the ELBO at $\\theta$, we evaluate it at the reparametarized $\\theta = \\mu + Bz + d \\circ \\epsilon$.\n",
    "The expectation with regard to $f$, $\\mathbb{E}_f$, can be estimated unbiasedly by generating one or more samples from $f$.\n",
    "\n",
    "Calculating the gradients delivers:\n",
    "- gradient w.r.t $\\mu$, i.e. mean of variational parameters $\\nabla_{\\mu} \\mathcal{L}(\\lambda) = \\mathbb{E}_f[\\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon)]$\n",
    "- gradient w.r.t. $B$, i.e. first component of covariance matrix of variational parameters $\\nabla_B \\mathcal{L}(\\lambda) = \\mathbb{E}_f[ \\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon)z^T + (BB^T + D^2)^{-1}(Bz+d \\circ \\epsilon)z^T]$\n",
    "- gradient w.r.t. $D = \\mathrm{diag}(d_1,...,d_n)$, i.e. the second component of covariance matrix of variational parameters $\\nabla_d \\mathcal{L}(\\lambda) = \\mathbb{E}_f[\\mathrm{diag}(\\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon) \\epsilon^T + (BB^T + D^2)^{-1}(Bz + d \\circ \\epsilon) \\epsilon^T] $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Initialize $\\lambda = \\lambda^{(0)} = (\\mu^{(0)}, B^{(0)}, d^{(0)})$, $t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load variables from training DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_coefficients_path = '../../data/commaai/extracted_coefficients/20201021_unrestr_gaussian_resampled/'\n",
    "B_zeta_path = str(extracted_coefficients_path + 'Bzeta/B_zeta.npy')\n",
    "beta_path = str(extracted_coefficients_path + 'beta/beta.csv')\n",
    "z_path = str(extracted_coefficients_path + 'Bzeta/tr_labels.npy')\n",
    "\n",
    "beta = np.genfromtxt(beta_path, delimiter=',')\n",
    "# B_zeta is a n x q matrix\n",
    "B_zeta = np.load(B_zeta_path)\n",
    "B_zeta = B_zeta.reshape(B_zeta.shape[0], beta.shape[0])\n",
    "tBB = B_zeta.T.dot(B_zeta)\n",
    "z = np.load(z_path) #[0:B_zeta.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p is the number of beta coefficients in the last hidden layer\n",
    "p = B_zeta.shape[1]\n",
    "\n",
    "\n",
    "# Lambda is a diagonal matrix of dimension p\n",
    "Lambda = np.diag(np.random.rand(p,))\n",
    "\n",
    "seed(679305)\n",
    "tau_start = 0.01\n",
    "\n",
    "# Set iteration counter to 0\n",
    "t = 0\n",
    "\n",
    "theta = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355543, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_zeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = B_zeta.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[ S(\\boldsymbol{x}, \\boldsymbol{\\theta}) = diag(s_1,...,s_n) \\\\]\n",
    "\n",
    "with $ s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^TP(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S(x, theta) is of dimension n x n\n",
    "W = np.array([B_zeta[i,:].dot(B_zeta[i,:]) for i in range(0, n)])\n",
    "S = np.sqrt(1/(1 + W*tau_start))\n",
    "S2 = S**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialize $\\lambda = \\lambda^{(0)} = (\\mu^{(0)},B^{(0)},d^{(0)}), \\, t = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m is number of variational parameters, which is \n",
    "# 2p (for each lambda_j and each beta_j)\n",
    "# plus the variational parameter for the prior on lambda\n",
    "m = p + 1\n",
    "\n",
    "# number of factors in the factored covariance representation\n",
    "k = m - 2\n",
    "\n",
    "mu_t = np.array([random() for i in range(0,m)]).reshape(m,1)\n",
    "# B is a lower triangle m x k matrix and is the first component of the \n",
    "# covariance matrix\n",
    "B_t = np.tril(np.random.rand(m,k))\n",
    "while not np.linalg.matrix_rank(B_t) == k:\n",
    "    B_t = np.tril(np.random.rand(m,k))\n",
    "\n",
    "# D is a diagonal matrix of dimension m x m and is the second component of the \n",
    "# covariance matrix\n",
    "D_t = np.diag(np.random.rand(m,))\n",
    "d_t = np.diag(D_t).reshape(m,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate $(\\epsilon^{(t)}, z^{(t)}) \\sim \\mathcal{N}(0,I) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_epsilon = np.repeat(0, m)\n",
    "mean_z = np.repeat(0, k)\n",
    "\n",
    "var_epsilon = np.diag(np.repeat(1,m))\n",
    "var_z = np.diag(np.repeat(1,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adadelta\n",
    "decay_rate = 0.95\n",
    "constant = 1e-7\n",
    "E_g2_t_1 = 0\n",
    "E_delta_x_2_1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_g2_t_1_mu = np.repeat(0, len(mu_t))\n",
    "E_delta_x_2_1_mu = np.repeat(0, len(mu_t))\n",
    "E_g2_t_1_B = np.zeros(B_t.shape)\n",
    "E_delta_x_2_1_B = np.zeros(B_t.shape)\n",
    "E_g2_t_1_d = np.repeat(0, len(d_t)).reshape(m,1)\n",
    "E_delta_x_2_1_d = np.repeat(0, len(d_t)).reshape(m,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adadelta_change(gradient, E_g2_t_1, E_delta_x_2_1, decay_rate = 0.99, constant = 10e-6):\n",
    "    # expected squared gradient for next iteration\n",
    "    E_g2_t = decay_rate*E_g2_t_1 + (1 - decay_rate)*(gradient**2)\n",
    "    # update for parameter\n",
    "    # should there be a minus or plus here ?????\n",
    "    delta_x =  (np.sqrt(E_delta_x_2_1 + constant)/np.sqrt(E_g2_t + constant))*gradient\n",
    "    # expected update for next iteration\n",
    "    E_delta_x_2 = decay_rate*E_delta_x_2_1 + (1 - decay_rate)*(delta_x**2)\n",
    "    return(delta_x, E_g2_t, E_delta_x_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16526/50000 [1:19:13<2:58:15,  3.13it/s]"
     ]
    }
   ],
   "source": [
    "lower_bounds = []\n",
    "all_varthetas = []\n",
    "t = 0\n",
    "iterations = 50000\n",
    "for i in tqdm(range(iterations)):\n",
    "    \n",
    "    # 1. Generate epsilon_t and z_t\n",
    "    z_t = hlp.generate_z(mean_z,var_z)\n",
    "    epsilon_t = hlp.generate_epsilon(mean_epsilon, var_epsilon)\n",
    "    \n",
    "    # 2. Draw from vartheta, what we generate are log values\n",
    "    # of lambda and tau -> have to transform them back to use them\n",
    "    vartheta_t = mu_t + B_t.dot(z_t) + (d_t*epsilon_t)\n",
    "    \n",
    "    beta_t = vartheta_t[0:p].reshape(p,)\n",
    "    betaBt_t = beta.dot(B_zeta.T)\n",
    "    u_t = vartheta_t[p]\n",
    "    # 3. Compute gradient of beta, lambda_j, and tau\n",
    "    gradient_h_t = hlp.Delta_theta(vartheta_t, B_zeta, n, z, p, tBB, betaBt_t, theta, beta_t, W)\n",
    "    \n",
    "    # Compute inverse with Woodbury formula.\n",
    "    inv = np.linalg.inv(D_t.dot(D_t))\n",
    "    inv2 = np.linalg.inv(np.identity(k) + B_t.T.dot(inv).dot(B_t))\n",
    "    BBD_inv = inv - multi_dot([inv, B_t, inv2, B_t.T, inv])\n",
    "    \n",
    "    # Compute gradients for the variational parameters mu, B, D\n",
    "    Delta_mu = hlp.Delta_mu(gradient_h_t, BBD_inv, z_t, d_t, epsilon_t, B_t)\n",
    "    Delta_B = hlp.Delta_B(B_zeta,n,z, p, B_t, gradient_h_t, z_t, D_t, d_t, epsilon_t, BBD_inv)\n",
    "    Delta_D = hlp.Delta_D(gradient_h_t, epsilon_t,D_t, d_t,p, BBD_inv)\n",
    "    \n",
    "    # 4. Adadelta Updates\n",
    "    update_mu, E_g2_t_1_mu, E_delta_x_2_1_mu = adadelta_change(Delta_mu, E_g2_t_1_mu, E_delta_x_2_1_mu, decay_rate = decay_rate, constant = constant)\n",
    "    update_B, E_g2_t_1_B, E_delta_x_2_1_B  = adadelta_change(Delta_B, E_g2_t_1_B, E_delta_x_2_1_B, decay_rate = decay_rate, constant = constant)\n",
    "    update_d, E_g2_t_1_d, E_delta_x_2_1_d = adadelta_change(Delta_D, E_g2_t_1_d, E_delta_x_2_1_d, decay_rate = decay_rate, constant = constant)\n",
    "    \n",
    "    # Update variables\n",
    "    '''rho = 0.9\n",
    "    mu_t = mu_t + rho*Delta_mu.reshape(m,1)\n",
    "    B_t = B_t + rho*Delta_B\n",
    "    B_t *= np.tri(*B_t.shape)\n",
    "    d_t = (d_t + rho*Delta_D)\n",
    "    D_t = np.diag(d_t.reshape(m,))'''\n",
    "    mu_t = mu_t + update_mu.reshape(m,1)\n",
    "    B_t = B_t + update_B\n",
    "    # set upper triangular elements to 0\n",
    "    B_t *= np.tri(*B_t.shape)\n",
    "    d_t = (d_t + update_d)\n",
    "    D_t = np.diag(d_t.reshape(m,))\n",
    "    \n",
    "    vartheta_t = mu_t + B_t.dot(z_t) + (d_t*epsilon_t)\n",
    "    vartheta_t_transf = vartheta_t.copy()\n",
    "    # 5. compute stopping criterion\n",
    "    beta_t = vartheta_t_transf[0:p].reshape(p,)\n",
    "    \n",
    "    # Lower bound L(lambda) = E[log(L_lambda - q_lambda]\n",
    "    log_h_t = hlp.log_density(z, u_t,  beta_t, B_zeta, Lambda, p, n, S, S2, tBB, theta, betaBt_t)\n",
    "    log_q_lambda_t = np.log(hlp.multivariate_normal(vartheta_t, m, mu_t, (B_t.dot(B_t.T) + D_t**2)))\n",
    "    # evidence lower bound\n",
    "    L_lambda = log_h_t - log_q_lambda_t\n",
    "    lower_bounds.append(L_lambda.item())\n",
    "    all_varthetas.append(vartheta_t)\n",
    "    \n",
    "    # increase time count\n",
    "    t = t+1\n",
    "    \n",
    "    # can also set lambda as the value over the last 10 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(lower_bounds)\n",
    "plt.yscale('symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17460938.111917824"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_varthetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2, dS2, ddS2, S = compdS(np.exp(u), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Delta_theta(vartheta_t, B, n, z, p, tBB, betaBt, theta, beta, W):\n",
    "    vartheta_new = vartheta_t.copy()\n",
    "    beta_t = vartheta_new[0:p].reshape(p,)\n",
    "    u = vartheta_new[p]\n",
    "    \n",
    "    S2, dS2, ddS2, S = compdS(np.exp(u), W)\n",
    "    \n",
    "    # Gradient w.r.t. beta\n",
    "    grad_beta = delta_beta(z, u, B, S, beta, tBB)\n",
    "    print(grad_beta.shape)\n",
    "    # Gradient w.r.t. tau\n",
    "    grad_tau = delta_tau(u, B, S2, dS2, z, beta, theta, betaBt).item()\n",
    "    return(np.append(grad_beta, grad_tau))\n",
    "\n",
    "Delta_theta(vartheta_t, B_zeta, n, z, p, tBB, betaBt_t, theta, beta_t, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../../data/density/03082020/lower_lambda_va.csv', lower_bounds, delimiter=\",\")\n",
    "np.savetxt('../../data/density/03082020/vartheta_final.csv', vartheta_t, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check convergence graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,t), lower_bounds, linewidth=0.1)\n",
    "plt.yscale('symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lower_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save $\\vartheta$ over last 10% of runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_10_percent = iterations*0.01\n",
    "vartheta_hat = mean(all_varthetas[last_10_percent:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../../data/bdd100k/density/03082020/vartheta_hat.csv', vartheta_hat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartheta_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_beta(z, u, B, S, beta, tBB):\n",
    "    return((z*(1/S)).dot(B) - beta.T.dot(tBB) - beta/np.exp(u))\n",
    "\n",
    "def delta_tau(u, B, S2, dS2, z, beta, theta, betaBt):\n",
    "    p = B.shape[1]\n",
    "    return( - 0.5*np.sum(dS2/S2) \n",
    "            - 0.5*np.sum((z**2)*(-dS2/(S2**2)))\n",
    "            + np.sum(betaBt*((-0.5*dS2/(S2**1.5))*z)) \n",
    "            - (0.5*p - 0.5)\n",
    "            + 0.5*(beta.T.dot(beta))/np.exp(u) \n",
    "            - 0.5*(np.exp(u)/theta)**0.5)\n",
    "\n",
    "def compdS(tau2, W):\n",
    "    tildeW = tau2*W\n",
    "    S2 = 1/(1+tildeW)\n",
    "    dS2 = -tildeW/((1+tildeW)**2)\n",
    "    ddS2 = -tildeW/((1+tildeW)**2) + 2*(tildeW**2)/((1+tildeW)**3)\n",
    "    \n",
    "    S = np.sqrt(S2)\n",
    "    return(S2, dS2, ddS2, S)\n",
    "\n",
    "def Delta_theta(vartheta_t, B, n, z, p, tBB, betaBt, theta, beta, W):\n",
    "    vartheta_new = vartheta_t.copy()\n",
    "    beta_t = vartheta_new[0:p].reshape(p,)\n",
    "    u = vartheta_new[p]\n",
    "    \n",
    "    S2, dS2, ddS2, S = compdS(np.exp(u), W)\n",
    "    \n",
    "    # Gradient w.r.t. beta\n",
    "    grad_beta = delta_beta(z, u, B, S, beta, tBB)\n",
    "    \n",
    "    # Gradient w.r.t. tau\n",
    "    grad_tau = delta_tau(u, B, S2, dS2, z, beta, theta, betaBt)\n",
    "    \n",
    "    return(np.append(grad_beta, grad_tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = vartheta_t[p]\n",
    "B = B_zeta\n",
    "S2, dS2, ddS2, S = compdS(np.exp(u), W)\n",
    "delta_tau(u, B, S2, dS2, z, beta, theta, betaBt_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_beta(z, u, B, S, beta, tBB).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Gradient w.r.t. beta\n",
    "grad_beta = delta_beta(z, u, B, S, beta, tBB)\n",
    "\n",
    "# Gradient w.r.t. tau\n",
    "grad_tau = delta_tau(u, B, S2, dS2, z, beta, theta, betaBt_t).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append(grad_beta, grad_tau).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_h_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betaBt_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.dot(B_zeta.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_zeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
