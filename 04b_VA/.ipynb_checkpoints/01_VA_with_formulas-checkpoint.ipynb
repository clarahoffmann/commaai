{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helpers as hlp\n",
    "import math\n",
    "from random import random, seed\n",
    "from numpy.linalg import multi_dot\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\"\n",
    "        src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML\"></script>\n",
    "\n",
    "## Model\n",
    "Linear model\n",
    "\\\\[ \\tilde{\\boldsymbol{Z}} = B_{\\boldsymbol{\\zeta}}(\\boldsymbol{x})\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\, \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2I) \\\\]\n",
    "\n",
    "and the transformed target variables follow a conditional normal distribution\n",
    "\n",
    "\\\\[ \\boldsymbol{Z} | \\boldsymbol{x}, \\sigma^2, \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{0}, R(\\boldsymbol{x}, \\boldsymbol{\\theta})^T) \\\\]\n",
    "with $ R(\\boldsymbol{x}, \\boldsymbol{\\theta}) = S(\\boldsymbol{x}, \\boldsymbol{\\theta})(I - B\\Omega B^T)^{-1}S(\\boldsymbol{x}, \\boldsymbol{\\theta})^T $\n",
    "\n",
    "and each Z_i has a marginal standard-normal distribution.\n",
    "\n",
    "The coefficient vector beta follows a conditional normal distribution\n",
    "\n",
    "\\\\[ \\boldsymbol{\\beta} | \\boldsymbol{x}, \\sigma^2, \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 P(\\boldsymbol{\\theta})^{-1}) \\\\]\n",
    "\n",
    "\n",
    "## Prior on copula parameters\n",
    "\n",
    "Horseshoe prior on coefficients\n",
    "\\\\[\\beta_j| \\lambda_j \\sim \\mathcal{N}(0,\\lambda_j^2) \\\\]\n",
    "with $\\pi_0(\\lambda_j | \\tau) = C^{+}(0,\\tau) $ and $ \\pi_0(\\tau) = C^{+}(0,1)$, where $C^{+}$ is the half-Cauchy distribution\n",
    "\n",
    "Then the vector of copula parameters is\n",
    "\\\\[\\boldsymbol{\\theta} = \\{ \\boldsymbol{\\lambda}, \\tau \\} \\\\]\n",
    "with \n",
    "\\\\[ \\boldsymbol{\\lambda} = (\\lambda_1,...\\lambda_p)^T \\\\]\n",
    "\n",
    "\\begin{equation}\n",
    "   P(\\boldsymbol{\\theta}) = diag(\\lambda_1^2,...\\lambda_p^2)^{-1}\n",
    "\\end{equation}\n",
    "and \n",
    "\\\\[ R(\\boldsymbol{x}, \\boldsymbol{\\theta}) = S(\\boldsymbol{x}, \\boldsymbol{\\theta}(I + B \\mathrm{diag}(\\lambda_1, ... \\lambda_p)^2 B ^T)S(\\boldsymbol{x}, \\boldsymbol{\\theta}) \\\\]\n",
    "\n",
    "\n",
    " - > so is P(theta) then just diag(phi_i)?\n",
    "Distribution of targets y\n",
    "\n",
    "\\\\[ p(\\boldsymbol{y}| \\boldsymbol{x}, \\boldsymbol{\\beta}, \\boldsymbol{\\theta}) = \\phi_n(\\boldsymbol{z};S(\\boldsymbol{x}, \\boldsymbol{\\theta})B_{\\boldsymbol{\\zeta}}\\boldsymbol{\\beta}, S(\\boldsymbol{x}, \\boldsymbol{\\theta})^2) \\prod_{i=1}^n \\frac{p_Y(y_i)}{\\phi_1(z_i)}, \\\\]\n",
    "with\n",
    "\n",
    "\\\\[ S(\\boldsymbol{x}, \\boldsymbol{\\theta}) = diag(s_1,...,s_n) \\\\]\n",
    "\n",
    "with $ s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^TP(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}}) $\n",
    "\n",
    "and specifically for the horseshoe prior case:\n",
    "\\\\[s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^T P(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}} \\\\]\n",
    "\n",
    "\n",
    "Now we want to optimize the ELBO which is given by\n",
    "\\\\[ \\mathcal{L}(\\lambda) = \\mathbb{E}_q[\\log h(\\vartheta) - \\log q_{\\lambda}(\\vartheta) ] \\\\]\n",
    "\n",
    "where $\\vartheta = \\{\\beta, \\theta \\}$ and $h(\\vartheta) = p(\\vartheta)p(y|\\vartheta)$.\n",
    "We want to estimate the augmented posteriors $\\beta, \\theta | \\boldsymbol{y}$\n",
    "\n",
    "\n",
    "## VA with factor covariance structure\n",
    "\n",
    "Choose an approximating family $ q_{\\lambda}(\\vartheta) $, in our case this is \n",
    "\\begin{equation}\n",
    "q_{\\lambda}(\\vartheta) = \\mathcal{N}(\\boldsymbol{\\mu}, BB^T + D^2),\n",
    "\\end{equation}\n",
    "with $d = \\{d_1,...d_m \\}$. The dimension of this distribution is $m$ and in the case of the horseshoe prior this is # of betas + 1 (tau) $= p+1$. The dimension of $B$ is $m \\times k$, where $k$ specifies the number of factors. To make computation easier $k$ should be much smaller than $m$. This implies that we can represent the dependency structure in the covariance matrix with a smaller number of latent variables, thus facilitating faster computation.\n",
    "\n",
    "How can we draw from this distribution?\n",
    "- first draw $(\\boldsymbol{z}, \\boldsymbol{\\epsilon}) \\sim \\mathcal{N}(0,I)$, where $\\boldsymbol{z}$ is of dimension $k \\times 1 $ and $\\boldsymbol{\\epsilon}$ is $m \\times 1$\n",
    "- then calculate $\\vartheta = \\boldsymbol{\\mu} + B\\boldsymbol{z} + d \\circ \\epsilon$, where $\\circ$ denotes the Hadamard product, i.e. the element by element multiplication of two vectors.\n",
    "\n",
    "By applying the reparametrization trick we can now change the  expectation with regard to $q_{\\lambda}(\\theta)$, namely $\\mathbb{E}_q$ to an expectation with regard to standard normal density of $z, \\epsilon$, which is denoted as $f(z, \\epsilon)$, which leads to the expectation $\\mathbb{E}_f$.\n",
    "Instead of evaluating the first time of the ELBO at $\\theta$, we evaluate it at the reparametarized $\\theta = \\mu + Bz + d \\circ \\epsilon$.\n",
    "The expectation with regard to $f$, $\\mathbb{E}_f$, can be estimated unbiasedly by generating one or more samples from $f$.\n",
    "\n",
    "Calculating the gradients delivers:\n",
    "- gradient w.r.t $\\mu$, i.e. mean of variational parameters $\\nabla_{\\mu} \\mathcal{L}(\\lambda) = \\mathbb{E}_f[\\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon)]$\n",
    "- gradient w.r.t. $B$, i.e. first component of covariance matrix of variational parameters $\\nabla_B \\mathcal{L}(\\lambda) = \\mathbb{E}_f[ \\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon)z^T + (BB^T + D^2)^{-1}(Bz+d \\circ \\epsilon)z^T]$\n",
    "- gradient w.r.t. $D = \\mathrm{diag}(d_1,...,d_n)$, i.e. the second component of covariance matrix of variational parameters $\\nabla_d \\mathcal{L}(\\lambda) = \\mathbb{E}_f[\\mathrm{diag}(\\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon) \\epsilon^T + (BB^T + D^2)^{-1}(Bz + d \\circ \\epsilon) \\epsilon^T] $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Initialize $\\lambda = \\lambda^{(0)} = (\\mu^{(0)}, B^{(0)}, d^{(0)})$, $t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load variables from training DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_coefficients_path = '../../data/commaai/extracted_coefficients/20201021_unrestr_gaussian_resampled/'\n",
    "B_zeta_path = str(extracted_coefficients_path + 'Bzeta/B_zeta.npy')\n",
    "beta_path = str(extracted_coefficients_path + 'beta/beta.csv')\n",
    "z_path = str(extracted_coefficients_path + 'Bzeta/tr_labels.npy')\n",
    "\n",
    "beta = np.genfromtxt(beta_path, delimiter=',')\n",
    "# B_zeta is a n x q matrix\n",
    "B_zeta = np.load(B_zeta_path)\n",
    "B_zeta = B_zeta.reshape(B_zeta.shape[0], beta.shape[0])\n",
    "tBB = B_zeta.T.dot(B_zeta)\n",
    "z = np.load(z_path) #[0:B_zeta.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p is the number of beta coefficients in the last hidden layer\n",
    "p = B_zeta.shape[1]\n",
    "\n",
    "\n",
    "# Lambda is a diagonal matrix of dimension p\n",
    "Lambda = np.diag(np.random.rand(p,))\n",
    "\n",
    "seed(679305)\n",
    "tau_start = 0.01\n",
    "\n",
    "# Set iteration counter to 0\n",
    "t = 0\n",
    "\n",
    "theta = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355543, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_zeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = B_zeta.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[ S(\\boldsymbol{x}, \\boldsymbol{\\theta}) = diag(s_1,...,s_n) \\\\]\n",
    "\n",
    "with $ s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^TP(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S(x, theta) is of dimension n x n\n",
    "W = np.array([B_zeta[i,:].dot(B_zeta[i,:]) for i in range(0, n)])\n",
    "S = np.sqrt(1/(1 + W*tau_start))\n",
    "S2 = S**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialize $\\lambda = \\lambda^{(0)} = (\\mu^{(0)},B^{(0)},d^{(0)}), \\, t = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m is number of variational parameters, which is \n",
    "# 2p (for each lambda_j and each beta_j)\n",
    "# plus the variational parameter for the prior on lambda\n",
    "m = p + 1\n",
    "\n",
    "# number of factors in the factored covariance representation\n",
    "k = m - 2\n",
    "\n",
    "mu_t = np.array([random() for i in range(0,m)]).reshape(m,1)\n",
    "# B is a lower triangle m x k matrix and is the first component of the \n",
    "# covariance matrix\n",
    "B_t = np.tril(np.random.rand(m,k))\n",
    "while not np.linalg.matrix_rank(B_t) == k:\n",
    "    B_t = np.tril(np.random.rand(m,k))\n",
    "\n",
    "# D is a diagonal matrix of dimension m x m and is the second component of the \n",
    "# covariance matrix\n",
    "D_t = np.diag(np.random.rand(m,))\n",
    "d_t = np.diag(D_t).reshape(m,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate $(\\epsilon^{(t)}, z^{(t)}) \\sim \\mathcal{N}(0,I) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_epsilon = np.repeat(0, m)\n",
    "mean_z = np.repeat(0, k)\n",
    "\n",
    "var_epsilon = np.diag(np.repeat(1,m))\n",
    "var_z = np.diag(np.repeat(1,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adadelta\n",
    "decay_rate = 0.95\n",
    "constant = 1e-7\n",
    "E_g2_t_1 = 0\n",
    "E_delta_x_2_1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_g2_t_1_mu = np.repeat(0, len(mu_t))\n",
    "E_delta_x_2_1_mu = np.repeat(0, len(mu_t))\n",
    "E_g2_t_1_B = np.zeros(B_t.shape)\n",
    "E_delta_x_2_1_B = np.zeros(B_t.shape)\n",
    "E_g2_t_1_d = np.repeat(0, len(d_t)).reshape(m,1)\n",
    "E_delta_x_2_1_d = np.repeat(0, len(d_t)).reshape(m,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adadelta_change(gradient, E_g2_t_1, E_delta_x_2_1, decay_rate = 0.99, constant = 10e-6):\n",
    "    # expected squared gradient for next iteration\n",
    "    E_g2_t = decay_rate*E_g2_t_1 + (1 - decay_rate)*(gradient**2)\n",
    "    # update for parameter\n",
    "    # should there be a minus or plus here ?????\n",
    "    delta_x =  (np.sqrt(E_delta_x_2_1 + constant)/np.sqrt(E_g2_t + constant))*gradient\n",
    "    # expected update for next iteration\n",
    "    E_delta_x_2 = decay_rate*E_delta_x_2_1 + (1 - decay_rate)*(delta_x**2)\n",
    "    return(delta_x, E_g2_t, E_delta_x_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/50000 [00:02<4:54:28,  2.83it/s]"
     ]
    }
   ],
   "source": [
    "lower_bounds = []\n",
    "all_varthetas = []\n",
    "t = 0\n",
    "iterations = 50000\n",
    "for i in tqdm(range(iterations)):\n",
    "    \n",
    "    # 1. Generate epsilon_t and z_t\n",
    "    z_t = hlp.generate_z(mean_z,var_z)\n",
    "    epsilon_t = hlp.generate_epsilon(mean_epsilon, var_epsilon)\n",
    "    \n",
    "    # 2. Draw from vartheta, what we generate are log values\n",
    "    # of lambda and tau -> have to transform them back to use them\n",
    "    vartheta_t = mu_t + B_t.dot(z_t) + (d_t*epsilon_t)\n",
    "    \n",
    "    beta_t = vartheta_t[0:p].reshape(p,)\n",
    "    betaBt_t = beta_t.dot(B_zeta.T)\n",
    "    \n",
    "    # 3. Compute gradient of beta, lambda_j, and tau\n",
    "    gradient_h_t = hlp.Delta_theta(vartheta_t, B_zeta, n, z, p, tBB, betaBt_t, theta, W)\n",
    "    \n",
    "    # Compute inverse with Woodbury formula.\n",
    "    inv = np.linalg.inv(D_t.dot(D_t))\n",
    "    inv2 = np.linalg.inv(np.identity(k) + B_t.T.dot(inv).dot(B_t))\n",
    "    BBD_inv = inv - multi_dot([inv, B_t, inv2, B_t.T, inv])\n",
    "    \n",
    "    # Compute gradients for the variational parameters mu, B, D\n",
    "    Delta_mu = hlp.Delta_mu(gradient_h_t, BBD_inv, z_t, d_t, epsilon_t, B_t)\n",
    "    Delta_B = hlp.Delta_B(B_zeta,n,z, p, B_t, gradient_h_t, z_t, D_t, d_t, epsilon_t, BBD_inv)\n",
    "    Delta_D = hlp.Delta_D(gradient_h_t, epsilon_t,D_t, d_t,p, BBD_inv)\n",
    "    \n",
    "    # 4. Adadelta Updates\n",
    "    update_mu, E_g2_t_1_mu, E_delta_x_2_1_mu = adadelta_change(Delta_mu, E_g2_t_1_mu, E_delta_x_2_1_mu, decay_rate = decay_rate, constant = constant)\n",
    "    update_B, E_g2_t_1_B, E_delta_x_2_1_B  = adadelta_change(Delta_B, E_g2_t_1_B, E_delta_x_2_1_B, decay_rate = decay_rate, constant = constant)\n",
    "    update_d, E_g2_t_1_d, E_delta_x_2_1_d = adadelta_change(Delta_D, E_g2_t_1_d, E_delta_x_2_1_d, decay_rate = decay_rate, constant = constant)\n",
    "    \n",
    "    # Update variables\n",
    "    '''rho = 0.9\n",
    "    mu_t = mu_t + rho*Delta_mu.reshape(m,1)\n",
    "    B_t = B_t + rho*Delta_B\n",
    "    B_t *= np.tri(*B_t.shape)\n",
    "    d_t = (d_t + rho*Delta_D)\n",
    "    D_t = np.diag(d_t.reshape(m,))'''\n",
    "    mu_t = mu_t + update_mu.reshape(m,1)\n",
    "    B_t = B_t + update_B\n",
    "    # set upper triangular elements to 0\n",
    "    B_t *= np.tri(*B_t.shape)\n",
    "    d_t = (d_t + update_d)\n",
    "    D_t = np.diag(d_t.reshape(m,))\n",
    "    \n",
    "    vartheta_t = mu_t + B_t.dot(z_t) + (d_t*epsilon_t)\n",
    "    vartheta_t_transf = vartheta_t.copy()\n",
    "    \n",
    "    # 5. compute stopping criterion\n",
    "    beta_t = vartheta_t_transf[0:p].reshape(p,)\n",
    "    u_t = vartheta_t_transf[p]\n",
    "    betaBt_t = beta_t.dot(B_zeta.T) \n",
    "    \n",
    "    # Lower bound L(lambda) = E[log(L_lambda - q_lambda]\n",
    "    log_h_t = hlp.log_density(z, u_t,  beta_t, B_zeta, p, n, S, S2, tBB, theta, betaBt_t)\n",
    "    log_q_lambda_t = np.log(hlp.multivariate_normal(vartheta_t, m, mu_t, (B_t.dot(B_t.T) + D_t**2)))\n",
    "    \n",
    "    # evidence lower bound\n",
    "    L_lambda = log_h_t - log_q_lambda_t\n",
    "    lower_bounds.append(L_lambda.item())\n",
    "    all_varthetas.append(vartheta_t)\n",
    "    \n",
    "    # increase time count\n",
    "    t = t+1\n",
    "    \n",
    "    # can also set lambda as the value over the last 10 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(lower_bounds)\n",
    "plt.yscale('symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.mean(np.array(all_varthetas).reshape(24026, 11)[:, 0:p], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_varthetas).reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data/commaai/va/unfiltered_gaussian_resampled/Ridge/lower_bounds.npy', lower_bounds)\n",
    "np.save('../../data/commaai/va/unfiltered_gaussian_resampled/Ridge/vartheta.npy', np.array(all_varthetas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check convergence graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,t), lower_bounds, linewidth=0.1)\n",
    "plt.yscale('symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lower_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(beta, beta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save $\\vartheta$ over last 10% of runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_10_percent = iterations*0.01\n",
    "vartheta_hat = mean(all_varthetas[last_10_percent:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../../../data/commaai/va/unfiltered_gaussian_resampled/Ridgevartheta_hat.csv', vartheta_hat, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
