{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helpers as hlp\n",
    "import math\n",
    "from random import random, seed\n",
    "from numpy.linalg import multi_dot\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\"\n",
    "        src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML\"></script>\n",
    "\n",
    "## Model\n",
    "Linear model\n",
    "\\\\[ \\tilde{\\boldsymbol{Z}} = B_{\\boldsymbol{\\zeta}}(\\boldsymbol{x})\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\, \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2I) \\\\]\n",
    "\n",
    "and the transformed target variables follow a conditional normal distribution\n",
    "\n",
    "\\\\[ \\boldsymbol{Z} | \\boldsymbol{x}, \\sigma^2, \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{0}, R(\\boldsymbol{x}, \\boldsymbol{\\theta})^T) \\\\]\n",
    "with $ R(\\boldsymbol{x}, \\boldsymbol{\\theta}) = S(\\boldsymbol{x}, \\boldsymbol{\\theta})(I - B\\Omega B^T)^{-1}S(\\boldsymbol{x}, \\boldsymbol{\\theta})^T $\n",
    "\n",
    "and each Z_i has a marginal standard-normal distribution.\n",
    "\n",
    "The coefficient vector beta follows a conditional normal distribution\n",
    "\n",
    "\\\\[ \\boldsymbol{\\beta} | \\boldsymbol{x}, \\sigma^2, \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 P(\\boldsymbol{\\theta})^{-1}) \\\\]\n",
    "\n",
    "\n",
    "## Prior on copula parameters\n",
    "\n",
    "Horseshoe prior on coefficients\n",
    "\\\\[\\beta_j| \\lambda_j \\sim \\mathcal{N}(0,\\lambda_j^2) \\\\]\n",
    "with $\\pi_0(\\lambda_j | \\tau) = C^{+}(0,\\tau) $ and $ \\pi_0(\\tau) = C^{+}(0,1)$, where $C^{+}$ is the half-Cauchy distribution\n",
    "\n",
    "Then the vector of copula parameters is\n",
    "\\\\[\\boldsymbol{\\theta} = \\{ \\boldsymbol{\\lambda}, \\tau \\} \\\\]\n",
    "with \n",
    "\\\\[ \\boldsymbol{\\lambda} = (\\lambda_1,...\\lambda_p)^T \\\\]\n",
    "\n",
    "\\begin{equation}\n",
    "   P(\\boldsymbol{\\theta}) = diag(\\lambda_1^2,...\\lambda_p^2)^{-1}\n",
    "\\end{equation}\n",
    "and \n",
    "\\\\[ R(\\boldsymbol{x}, \\boldsymbol{\\theta}) = S(\\boldsymbol{x}, \\boldsymbol{\\theta}(I + B \\mathrm{diag}(\\lambda_1, ... \\lambda_p)^2 B ^T)S(\\boldsymbol{x}, \\boldsymbol{\\theta}) \\\\]\n",
    "\n",
    "\n",
    " - > so is P(theta) then just diag(phi_i)?\n",
    "Distribution of targets y\n",
    "\n",
    "\\\\[ p(\\boldsymbol{y}| \\boldsymbol{x}, \\boldsymbol{\\beta}, \\boldsymbol{\\theta}) = \\phi_n(\\boldsymbol{z};S(\\boldsymbol{x}, \\boldsymbol{\\theta})B_{\\boldsymbol{\\zeta}}\\boldsymbol{\\beta}, S(\\boldsymbol{x}, \\boldsymbol{\\theta})^2) \\prod_{i=1}^n \\frac{p_Y(y_i)}{\\phi_1(z_i)}, \\\\]\n",
    "with\n",
    "\n",
    "\\\\[ S(\\boldsymbol{x}, \\boldsymbol{\\theta}) = diag(s_1,...,s_n) \\\\]\n",
    "\n",
    "with $ s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^TP(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}}) $\n",
    "\n",
    "and specifically for the horseshoe prior case:\n",
    "\\\\[s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^T P(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}} \\\\]\n",
    "\n",
    "\n",
    "Now we want to optimize the ELBO which is given by\n",
    "\\\\[ \\mathcal{L}(\\lambda) = \\mathbb{E}_q[\\log h(\\vartheta) - \\log q_{\\lambda}(\\vartheta) ] \\\\]\n",
    "\n",
    "where $\\vartheta = \\{\\beta, \\theta \\}$ and $h(\\vartheta) = p(\\vartheta)p(y|\\vartheta)$.\n",
    "We want to estimate the augmented posteriors $\\beta, \\theta | \\boldsymbol{y}$\n",
    "\n",
    "\n",
    "## VA with factor covariance structure\n",
    "\n",
    "Choose an approximating family $ q_{\\lambda}(\\vartheta) $, in our case this is \n",
    "\\begin{equation}\n",
    "q_{\\lambda}(\\vartheta) = \\mathcal{N}(\\boldsymbol{\\mu}, BB^T + D^2),\n",
    "\\end{equation}\n",
    "with $d = \\{d_1,...d_m \\}$. The dimension of this distribution is $m$ and in the case of the horseshoe prior this is # of betas + 1 (tau) $= p+1$. The dimension of $B$ is $m \\times k$, where $k$ specifies the number of factors. To make computation easier $k$ should be much smaller than $m$. This implies that we can represent the dependency structure in the covariance matrix with a smaller number of latent variables, thus facilitating faster computation.\n",
    "\n",
    "How can we draw from this distribution?\n",
    "- first draw $(\\boldsymbol{z}, \\boldsymbol{\\epsilon}) \\sim \\mathcal{N}(0,I)$, where $\\boldsymbol{z}$ is of dimension $k \\times 1 $ and $\\boldsymbol{\\epsilon}$ is $m \\times 1$\n",
    "- then calculate $\\vartheta = \\boldsymbol{\\mu} + B\\boldsymbol{z} + d \\circ \\epsilon$, where $\\circ$ denotes the Hadamard product, i.e. the element by element multiplication of two vectors.\n",
    "\n",
    "By applying the reparametrization trick we can now change the  expectation with regard to $q_{\\lambda}(\\theta)$, namely $\\mathbb{E}_q$ to an expectation with regard to standard normal density of $z, \\epsilon$, which is denoted as $f(z, \\epsilon)$, which leads to the expectation $\\mathbb{E}_f$.\n",
    "Instead of evaluating the first time of the ELBO at $\\theta$, we evaluate it at the reparametarized $\\theta = \\mu + Bz + d \\circ \\epsilon$.\n",
    "The expectation with regard to $f$, $\\mathbb{E}_f$, can be estimated unbiasedly by generating one or more samples from $f$.\n",
    "\n",
    "Calculating the gradients delivers:\n",
    "- gradient w.r.t $\\mu$, i.e. mean of variational parameters $\\nabla_{\\mu} \\mathcal{L}(\\lambda) = \\mathbb{E}_f[\\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon)]$\n",
    "- gradient w.r.t. $B$, i.e. first component of covariance matrix of variational parameters $\\nabla_B \\mathcal{L}(\\lambda) = \\mathbb{E}_f[ \\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon)z^T + (BB^T + D^2)^{-1}(Bz+d \\circ \\epsilon)z^T]$\n",
    "- gradient w.r.t. $D = \\mathrm{diag}(d_1,...,d_n)$, i.e. the second component of covariance matrix of variational parameters $\\nabla_d \\mathcal{L}(\\lambda) = \\mathbb{E}_f[\\mathrm{diag}(\\nabla_{\\vartheta} \\log h(\\mu + Bz + d \\circ \\epsilon) \\epsilon^T + (BB^T + D^2)^{-1}(Bz + d \\circ \\epsilon) \\epsilon^T] $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Initialize $\\lambda = \\lambda^{(0)} = (\\mu^{(0)}, B^{(0)}, d^{(0)})$, $t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load variables from training DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_coefficients_path = '../../data/commaai/extracted_coefficients/20201021_unrestr_gaussian_resampled/'\n",
    "B_zeta_path = str(extracted_coefficients_path + 'Bzeta/B_zeta.npy')\n",
    "beta_path = str(extracted_coefficients_path + 'beta/beta.csv')\n",
    "z_path = str(extracted_coefficients_path + 'Bzeta/tr_labels.npy')\n",
    "\n",
    "beta = np.genfromtxt(beta_path, delimiter=',')\n",
    "# B_zeta is a n x q matrix\n",
    "B_zeta = np.load(B_zeta_path)\n",
    "B_zeta = B_zeta.reshape(B_zeta.shape[0], beta.shape[0])\n",
    "tBB = B_zeta.T.dot(B_zeta)\n",
    "z = np.load(z_path) #[0:B_zeta.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p is the number of beta coefficients in the last hidden layer\n",
    "p = B_zeta.shape[1]\n",
    "\n",
    "\n",
    "# Lambda is a diagonal matrix of dimension p\n",
    "Lambda = np.diag(np.random.rand(p,))\n",
    "\n",
    "seed(679305)\n",
    "tau_start = 0.01\n",
    "\n",
    "# Set iteration counter to 0\n",
    "t = 0\n",
    "\n",
    "theta = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355543, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_zeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = B_zeta.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[ S(\\boldsymbol{x}, \\boldsymbol{\\theta}) = diag(s_1,...,s_n) \\\\]\n",
    "\n",
    "with $ s_i = (1+ \\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^TP(\\boldsymbol{\\theta})^{-1}\\psi_{\\boldsymbol{\\zeta}}(\\boldsymbol{x}_i)^{-\\frac{1}{2}}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S(x, theta) is of dimension n x n\n",
    "W = np.array([B_zeta[i,:].dot(B_zeta[i,:]) for i in range(0, n)])\n",
    "S = np.sqrt(1/(1 + W*tau_start))\n",
    "S2 = S**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialize $\\lambda = \\lambda^{(0)} = (\\mu^{(0)},B^{(0)},d^{(0)}), \\, t = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m is number of variational parameters, which is \n",
    "# 2p (for each lambda_j and each beta_j)\n",
    "# plus the variational parameter for the prior on lambda\n",
    "m = p + 1\n",
    "\n",
    "# number of factors in the factored covariance representation\n",
    "k = m - 2\n",
    "\n",
    "mu_t = np.array([random() for i in range(0,m)]).reshape(m,1)\n",
    "# B is a lower triangle m x k matrix and is the first component of the \n",
    "# covariance matrix\n",
    "B_t = np.tril(np.random.rand(m,k))\n",
    "while not np.linalg.matrix_rank(B_t) == k:\n",
    "    B_t = np.tril(np.random.rand(m,k))\n",
    "\n",
    "# D is a diagonal matrix of dimension m x m and is the second component of the \n",
    "# covariance matrix\n",
    "D_t = np.diag(np.random.rand(m,))\n",
    "d_t = np.diag(D_t).reshape(m,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate $(\\epsilon^{(t)}, z^{(t)}) \\sim \\mathcal{N}(0,I) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_epsilon = np.repeat(0, m)\n",
    "mean_z = np.repeat(0, k)\n",
    "\n",
    "var_epsilon = np.diag(np.repeat(1,m))\n",
    "var_z = np.diag(np.repeat(1,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adadelta\n",
    "decay_rate = 0.95\n",
    "constant = 1e-7\n",
    "E_g2_t_1 = 0\n",
    "E_delta_x_2_1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_g2_t_1_mu = np.repeat(0, len(mu_t))\n",
    "E_delta_x_2_1_mu = np.repeat(0, len(mu_t))\n",
    "E_g2_t_1_B = np.zeros(B_t.shape)\n",
    "E_delta_x_2_1_B = np.zeros(B_t.shape)\n",
    "E_g2_t_1_d = np.repeat(0, len(d_t)).reshape(m,1)\n",
    "E_delta_x_2_1_d = np.repeat(0, len(d_t)).reshape(m,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adadelta_change(gradient, E_g2_t_1, E_delta_x_2_1, decay_rate = 0.99, constant = 10e-6):\n",
    "    # expected squared gradient for next iteration\n",
    "    E_g2_t = decay_rate*E_g2_t_1 + (1 - decay_rate)*(gradient**2)\n",
    "    # update for parameter\n",
    "    # should there be a minus or plus here ?????\n",
    "    delta_x =  (np.sqrt(E_delta_x_2_1 + constant)/np.sqrt(E_g2_t + constant))*gradient\n",
    "    # expected update for next iteration\n",
    "    E_delta_x_2 = decay_rate*E_delta_x_2_1 + (1 - decay_rate)*(delta_x**2)\n",
    "    return(delta_x, E_g2_t, E_delta_x_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/50000 [00:10<8:29:21,  1.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a2d41cf958a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Lower bound L(lambda) = E[log(L_lambda - q_lambda]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mlog_h_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_t\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbeta_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_zeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtBB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetaBt_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mlog_q_lambda_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvartheta_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mD_t\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/commaai_code/04b_VA/helpers.py\u001b[0m in \u001b[0;36mlog_density\u001b[0;34m(z, u, beta, B, p, n, S, S2, tBB, theta, betaBt)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlog_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtBB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetaBt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     return (\n\u001b[0;32m---> 65\u001b[0;31m           \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m           \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mS2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbetaBt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m           \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtBB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lower_bounds = []\n",
    "all_varthetas = []\n",
    "mu_ts = []\n",
    "d_ts = []\n",
    "B_ts = []\n",
    "t = 0\n",
    "iterations = 50000\n",
    "for i in tqdm(range(iterations)):\n",
    "    \n",
    "    # 1. Generate epsilon_t and z_t\n",
    "    z_t = hlp.generate_z(mean_z,var_z)\n",
    "    epsilon_t = hlp.generate_epsilon(mean_epsilon, var_epsilon)\n",
    "    \n",
    "    # 2. Draw from vartheta, what we generate are log values\n",
    "    # of lambda and tau -> have to transform them back to use them\n",
    "    vartheta_t = mu_t + B_t.dot(z_t) + (d_t*epsilon_t)\n",
    "    \n",
    "    beta_t = vartheta_t[0:p].reshape(p,)\n",
    "    betaBt_t = beta_t.dot(B_zeta.T)\n",
    "    \n",
    "    # 3. Compute gradient of beta, lambda_j, and tau\n",
    "    gradient_h_t = hlp.Delta_theta(vartheta_t, B_zeta, n, z, p, tBB, betaBt_t, theta, W)\n",
    "    \n",
    "    # Compute inverse with Woodbury formula.\n",
    "    inv = np.linalg.inv(D_t.dot(D_t))\n",
    "    inv2 = np.linalg.inv(np.identity(k) + B_t.T.dot(inv).dot(B_t))\n",
    "    BBD_inv = inv - multi_dot([inv, B_t, inv2, B_t.T, inv])\n",
    "    \n",
    "    # Compute gradients for the variational parameters mu, B, D\n",
    "    Delta_mu = hlp.Delta_mu(gradient_h_t, BBD_inv, z_t, d_t, epsilon_t, B_t)\n",
    "    Delta_B = hlp.Delta_B(B_zeta,n,z, p, B_t, gradient_h_t, z_t, D_t, d_t, epsilon_t, BBD_inv)\n",
    "    Delta_D = hlp.Delta_D(gradient_h_t, epsilon_t,D_t, d_t,p, BBD_inv)\n",
    "    \n",
    "    # 4. Adadelta Updates\n",
    "    update_mu, E_g2_t_1_mu, E_delta_x_2_1_mu = adadelta_change(Delta_mu, E_g2_t_1_mu, E_delta_x_2_1_mu, decay_rate = decay_rate, constant = constant)\n",
    "    update_B, E_g2_t_1_B, E_delta_x_2_1_B  = adadelta_change(Delta_B, E_g2_t_1_B, E_delta_x_2_1_B, decay_rate = decay_rate, constant = constant)\n",
    "    update_d, E_g2_t_1_d, E_delta_x_2_1_d = adadelta_change(Delta_D, E_g2_t_1_d, E_delta_x_2_1_d, decay_rate = decay_rate, constant = constant)\n",
    "    \n",
    "    # Update variables\n",
    "    '''rho = 0.9\n",
    "    mu_t = mu_t + rho*Delta_mu.reshape(m,1)\n",
    "    B_t = B_t + rho*Delta_B\n",
    "    B_t *= np.tri(*B_t.shape)\n",
    "    d_t = (d_t + rho*Delta_D)\n",
    "    D_t = np.diag(d_t.reshape(m,))'''\n",
    "    mu_t = mu_t + update_mu.reshape(m,1)\n",
    "    B_t = B_t + update_B\n",
    "    # set upper triangular elements to 0\n",
    "    B_t *= np.tri(*B_t.shape)\n",
    "    d_t = (d_t + update_d)\n",
    "    D_t = np.diag(d_t.reshape(m,))\n",
    "    \n",
    "    vartheta_t = mu_t + B_t.dot(z_t) + (d_t*epsilon_t)\n",
    "    vartheta_t_transf = vartheta_t.copy()\n",
    "    \n",
    "    # 5. compute stopping criterion\n",
    "    beta_t = vartheta_t_transf[0:p].reshape(p,)\n",
    "    u_t = vartheta_t_transf[p]\n",
    "    betaBt_t = beta_t.dot(B_zeta.T) \n",
    "    \n",
    "    # Lower bound L(lambda) = E[log(L_lambda - q_lambda]\n",
    "    log_h_t = hlp.log_density(z, u_t,  beta_t, B_zeta, p, n, S, S2, tBB, theta, betaBt_t)\n",
    "    log_q_lambda_t = np.log(hlp.multivariate_normal(vartheta_t, m, mu_t, (B_t.dot(B_t.T) + D_t**2)))\n",
    "    \n",
    "    # evidence lower bound\n",
    "    L_lambda = log_h_t - log_q_lambda_t\n",
    "    lower_bounds.append(L_lambda.item())\n",
    "    all_varthetas.append(vartheta_t)\n",
    "    \n",
    "    # increase time count\n",
    "    t = t+1\n",
    "    \n",
    "    # can also set lambda as the value over the last 10 steps\n",
    "    mu_ts.append(mu_t)\n",
    "    d_ts.append(d_t)\n",
    "    B_ts.append(B_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1776e19970>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(lower_bounds)\n",
    "#plt.yscale('symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.mean(np.array(all_varthetas).reshape(24026, 11)[:, 0:p], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_varthetas).reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('../../data/commaai/va/filtered_gaussian_resampled/Ridge/lower_bounds_delete.npy', lower_bounds)\n",
    "#np.save('../../data/commaai/va/filtered_gaussian_resampled/Ridge/vartheta_delete.npy', np.array(all_varthetas))\n",
    "np.save('../../data/commaai/va/filtered_gaussian_resampled/Ridge/mu_ts_delete.npy', mu_ts)\n",
    "np.save('../../data/commaai/va/filtered_gaussian_resampled/Ridge/d_ts_delete.npy', d_ts)\n",
    "np.save('../../data/commaai/va/filtered_gaussian_resampled/Ridge/B_ts_delete.npy', B_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check convergence graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,t), lower_bounds, linewidth=0.1)\n",
    "plt.yscale('symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lower_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(beta, beta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save $\\vartheta$ over last 10% of runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_10_percent = iterations*0.01\n",
    "vartheta_hat = mean(all_varthetas[last_10_percent:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../../../data/commaai/va/unfiltered_gaussian_resampled/Ridgevartheta_hat.csv', vartheta_hat, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
