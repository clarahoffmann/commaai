{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = '../../data/commaai/train_files_from_bag/'\n",
    "bag_path = '../../data/commaai/train_bags_2/'\n",
    "density_path = '../../data/commaai/density/fastkde_density.csv'\n",
    "density = pd.read_csv(density_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an = pd.read_csv('../../data/commaai/train_files_from_bag/yaws.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "nb_all = []\n",
    "for i in range(0, len(all_files)):\n",
    "    nb = len(all_files[i])\n",
    "    print('bag ' +  str(i) +  ' has ' + str(nb) + ' observations')\n",
    "    nb_all.append(nb)\n",
    "    count += nb\n",
    "print('overall number of samples: ' + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(all_files[2][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(nb_all[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('test_files_run2.npy')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_int64(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def wrap_bytes(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "def wrap_float(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "def find_closest_element(y: float, arr: np.ndarray):\n",
    "    index = np.searchsorted(arr,y)\n",
    "    if (index >= 1) & (index < arr.shape[0]):\n",
    "        res = [arr[index - 1], arr[index]]\n",
    "    elif (index < arr.shape[0]):\n",
    "        return np.array(index)\n",
    "    else:\n",
    "        return np.array(index - 1)\n",
    "\n",
    "    if res[0] == res[1]:\n",
    "        return np.array(index - 1)\n",
    "    else:\n",
    "        diff_pre = np.abs(y-res[0])\n",
    "        diff_aft = np.abs(y-res[1])\n",
    "        if diff_pre == diff_aft:\n",
    "            return np.array(index - 1), \n",
    "        else:\n",
    "            return index - 1 if diff_pre < diff_aft else index\n",
    "def Fy(y, density, density_type = 'fast_kde' ):\n",
    "    integral = density.loc[find_closest_element(y, density['axes']),'cdf']\n",
    "    return(integral)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_minus_1 = np.zeros(20)\n",
    "n = 0\n",
    "while np.sum(counts) < 100000:\n",
    "    \n",
    "    #start new shard when more than 256 images have been written\n",
    "    if (np.sum(counts_minus_1 - counts) > 256):\n",
    "        n += 1\n",
    "    \n",
    "    out_path_shard = str(destination + str(n) + '_new2.tfrecords')\n",
    "\n",
    "    print(out_path_shard)\n",
    "            # start tfrecords writer\n",
    "\n",
    "    with tf.io.TFRecordWriter(out_path_shard) as writer:\n",
    "\n",
    "         for i in range(0,len(all_files)):\n",
    "\n",
    "            if all_files[i]: \n",
    "\n",
    "                # draw new sample if bin is not full yet\n",
    "                if counts[i] <= max_bins[i]:\n",
    "\n",
    "                    # load random example from bag i\n",
    "                    current_file = str(random.choice(all_files[i]))\n",
    "                    x = np.load(current_file, allow_pickle=True)\n",
    "                    img = x[0]\n",
    "                    label = x[1]\n",
    "                    tr_label = x[2]\n",
    "\n",
    "                    # write to shard\n",
    "                    img_bytes = img.tostring()\n",
    "                    rows = img.shape[0]\n",
    "                    cols = img.shape[1]\n",
    "                    depth = img.shape[2]\n",
    "\n",
    "\n",
    "                    # save image and label in dictionary\n",
    "                    data = \\\n",
    "                            {\n",
    "                                'image': wrap_bytes(img_bytes),\n",
    "                                'label': wrap_float(label),\n",
    "                                'rows': wrap_int64(rows),\n",
    "                                'cols': wrap_int64(cols),\n",
    "                                'depth': wrap_int64(depth),\n",
    "                                'tr_label': wrap_float(tr_label)\n",
    "\n",
    "                            }\n",
    "                    feature = tf.train.Features(feature=data)\n",
    "\n",
    "                    # Wrap again as a TensorFlow Example.\n",
    "                    example = tf.train.Example(features=feature)\n",
    "\n",
    "                    serialized = example.SerializeToString()\n",
    "\n",
    "\n",
    "                    with open(str(destination + 'yaws.csv'), 'a') as csvfile:\n",
    "                        csvfile.write(\"%s\\n\" % label)\n",
    "\n",
    "                    # Write the serialized data to the TFRecords file.\n",
    "                    writer.write(serialized)\n",
    "\n",
    "                    counts[i] += 1\n",
    "\n",
    "                    # mirror image\n",
    "                    img = np.fliplr(img)\n",
    "                    label = - label\n",
    "                    tr_label = norm.ppf(Fy(label, density))\n",
    "\n",
    "                    # write to shard\n",
    "                    img_bytes = img.tostring()\n",
    "                    rows = img.shape[0]\n",
    "                    cols = img.shape[1]\n",
    "                    depth = img.shape[2]\n",
    "\n",
    "\n",
    "                    # save image and label in dictionary\n",
    "                    data = \\\n",
    "                            {\n",
    "                                'image': wrap_bytes(img_bytes),\n",
    "                                'label': wrap_float(label),\n",
    "                                'rows': wrap_int64(rows),\n",
    "                                'cols': wrap_int64(cols),\n",
    "                                'depth': wrap_int64(depth),\n",
    "                                'tr_label': wrap_float(tr_label)\n",
    "\n",
    "                            }\n",
    "                    feature = tf.train.Features(feature=data)\n",
    "\n",
    "                    # Wrap again as a TensorFlow Example.\n",
    "                    example = tf.train.Example(features=feature)\n",
    "\n",
    "                    serialized = example.SerializeToString()\n",
    "\n",
    "\n",
    "                    with open(str(destination + 'yaws.csv'), 'a') as csvfile:\n",
    "                        csvfile.write(\"%s\\n\" % label)\n",
    "\n",
    "                    # Write the serialized data to the TFRecords file.\n",
    "                    writer.write(serialized)\n",
    "\n",
    "                    counts[i] += 1\n",
    "            \n",
    "            counts_minus_1 = counts.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_bins = [10000, 10000, 10000, 7500, 5000, 2500, 1000, 1000, 1000, 1000,\n",
    "           500, 500, 500, 500, 500, 500, 500, 500, 500, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(current_file, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(max_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x[0]\n",
    "angle = x[1]\n",
    "tr_angle = x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img/169)\n",
    "print('angle: ' + str(angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
